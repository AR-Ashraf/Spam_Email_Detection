{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spam Identifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx8cZcDrCzlF"
      },
      "source": [
        "# **Let's Identify Spam Email!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKKH-RodDPF2"
      },
      "source": [
        "### Have you ever thought how gmail, yahoo, outlook etc are filtering out the spam emails from your recieved emails? \n",
        "\n",
        "#### Well, they use machine learning algorithms for this. As we want to know if an email is spam or not, so we are gonna use binary classifier algorithm. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S4Pep7nELKY"
      },
      "source": [
        "To sove this problem we will try to understand the problems and solve it step by step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdtW21iKDJB-"
      },
      "source": [
        "# All the required libraries \n",
        "import google.colab\n",
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "import email\n",
        "import email.policy\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from html import unescape\n",
        "import nltk\n",
        "import urlextract\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4uAxq6oF5sJ"
      },
      "source": [
        "#### **Step 1: Fetch the Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN5FJqTCFvGN"
      },
      "source": [
        "URL_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
        "HAM_URL = URL_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
        "SPAM_URL = URL_ROOT + \"20030228_spam.tar.bz2\"\n",
        "SPAM_PATH = os.path.join(\"Dataset\", \"Spams\")\n",
        "\n",
        "def fetch_spam_data(ham_url=HAM_URL, spam_url=SPAM_URL, spam_path=SPAM_PATH):\n",
        "    if not os.path.isdir(spam_path):\n",
        "        os.makedirs(spam_path)\n",
        "    for filename, url in ((\"ham.tar.bz2\", ham_url), (\"spam.tar.bz2\", spam_url)):\n",
        "        path = os.path.join(spam_path, filename)\n",
        "        if not os.path.isfile(path):\n",
        "            urllib.request.urlretrieve(url, path)\n",
        "        tar_file = tarfile.open(path)\n",
        "        tar_file.extractall(path=spam_path)\n",
        "        tar_file.close()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUS9tplPHRfu"
      },
      "source": [
        "# Now load all data and emails\n",
        "fetch_spam_data()\n",
        "HAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\n",
        "SPAM_DIR = os.path.join(SPAM_PATH, \"spam\")\n",
        "ham_filenames = [name for name in sorted(os.listdir(HAM_DIR))if len(name) > 20]\n",
        "spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtq9Ho54H73W",
        "outputId": "2360423d-db2c-4a5a-c1ab-c4583e0fae2e"
      },
      "source": [
        "len(ham_filenames), len(spam_filenames)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2500, 500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4bzgdY9IBNG"
      },
      "source": [
        "# A function to parse all the emails we have loaded. We are using Python's library email to do this.\n",
        "# What it actually does is that, it labels the data present from an actual email.\n",
        "# We need to break the whole email into different segments like: from, to, body, date etc. This library helps us to do that.\n",
        "def parse_email(is_spam, filename, spam_path=SPAM_PATH):\n",
        "    directory = \"spam\" if is_spam else \"easy_ham\"\n",
        "    with open(os.path.join(spam_path, directory, filename), \"rb\") as f:\n",
        "        return email.parser.BytesParser(policy=email.policy.default).parse(f)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCRTpmZJJ00I"
      },
      "source": [
        "ham_emails = [parse_email(is_spam=False, filename=name) for name in ham_filenames]\n",
        "spam_emails = [parse_email(is_spam=True, filename=name) for name in spam_filenames]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8srMxsjOWXf"
      },
      "source": [
        "#### **Step 2: Understanding the Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl69OpdUQ6Jf"
      },
      "source": [
        "There are many contents in a sigle email. We need to understand those contents so that we can use them as labels and train our model to classigy based on each content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaGntQ9uKFmN"
      },
      "source": [
        "def get_email_content(email):\n",
        "    if isinstance(email, str):\n",
        "        return email\n",
        "    payload = email.get_payload()\n",
        "    if isinstance(payload, list):\n",
        "        return \"multipart({})\".format(\", \".join([\n",
        "            get_email_content(sub_email)\n",
        "            for sub_email in payload\n",
        "        ]))\n",
        "    else:\n",
        "        return email.get_content_type()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qY8-eqISkiC"
      },
      "source": [
        "# Now let's use counter library to count the contents of each email.\n",
        "def content_counter(emails):\n",
        "    contents = Counter()\n",
        "    for email in emails:\n",
        "        content = get_email_content(email)\n",
        "        contents[content] += 1\n",
        "    return contents"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vfUEr4nTzoC",
        "outputId": "e61ac35e-4eb0-4933-b84d-11475d2e7ce6"
      },
      "source": [
        "content_counter(ham_emails).most_common()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('text/plain', 2408),\n",
              " ('multipart(text/plain, application/pgp-signature)', 66),\n",
              " ('multipart(text/plain, text/html)', 8),\n",
              " ('multipart(text/plain, text/plain)', 4),\n",
              " ('multipart(text/plain)', 3),\n",
              " ('multipart(text/plain, application/octet-stream)', 2),\n",
              " ('multipart(text/plain, text/enriched)', 1),\n",
              " ('multipart(text/plain, application/ms-tnef, text/plain)', 1),\n",
              " ('multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)',\n",
              "  1),\n",
              " ('multipart(text/plain, video/mng)', 1),\n",
              " ('multipart(text/plain, multipart(text/plain))', 1),\n",
              " ('multipart(text/plain, application/x-pkcs7-signature)', 1),\n",
              " ('multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)',\n",
              "  1),\n",
              " ('multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))',\n",
              "  1),\n",
              " ('multipart(text/plain, application/x-java-applet)', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hK7q9P4-T20b",
        "outputId": "044bc1ac-7f99-4980-b2cc-b682ce9a63e0"
      },
      "source": [
        "content_counter(spam_emails).most_common()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('text/plain', 218),\n",
              " ('text/html', 183),\n",
              " ('multipart(text/plain, text/html)', 45),\n",
              " ('multipart(text/html)', 20),\n",
              " ('multipart(text/plain)', 19),\n",
              " ('multipart(multipart(text/html))', 5),\n",
              " ('multipart(text/plain, image/jpeg)', 3),\n",
              " ('multipart(text/html, application/octet-stream)', 2),\n",
              " ('multipart(text/plain, application/octet-stream)', 1),\n",
              " ('multipart(text/html, text/plain)', 1),\n",
              " ('multipart(multipart(text/html), application/octet-stream, image/jpeg)', 1),\n",
              " ('multipart(multipart(text/plain, text/html), image/gif)', 1),\n",
              " ('multipart/alternative', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n09ExJLWlMsG"
      },
      "source": [
        "We can see that most of the ham_emails are plain text and signed. But in case of spam_emails, none of the emails are signed and most of them are html."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcsH6dcunb_d"
      },
      "source": [
        "If you want to understand the data more then you can print out by using index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX0G5RAEnz0f"
      },
      "source": [
        "#### **Step 3: Procressing the Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa6RmW9Dkjd4"
      },
      "source": [
        "X = np.array(ham_emails + spam_emails, dtype=object)\n",
        "\n",
        "# We will set 0 for all the ham_emails and 1 for all the spam_emails and make an array of those zeros and ones.\n",
        "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g3U1d5DoU26",
        "outputId": "d57578b8-1837-42f5-be9a-3921ef065c8d"
      },
      "source": [
        "X"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([<email.message.EmailMessage object at 0x7fd9ee785110>,\n",
              "       <email.message.EmailMessage object at 0x7fd9ee785310>,\n",
              "       <email.message.EmailMessage object at 0x7fd9ee785250>, ...,\n",
              "       <email.message.EmailMessage object at 0x7fd9ed0b3710>,\n",
              "       <email.message.EmailMessage object at 0x7fd9ed0b4090>,\n",
              "       <email.message.EmailMessage object at 0x7fd9ed0bb490>],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08IRtIeRoYXk",
        "outputId": "31b312f5-f00d-45a3-ef34-c845ff184c94"
      },
      "source": [
        "y"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYKbrh_sqEpF"
      },
      "source": [
        "Now we will convert the html data into plain text so that we can train them too. For that we are using regex function here as we know the data structure. But there are some other dependencies: BeautifulSoup, html2text etc that can also be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls2ZJnPZobus"
      },
      "source": [
        "# unescape is used to remove html entities (such as &gt; or &nbsp;)\n",
        "def html_to_plain_text(html):\n",
        "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
        "    text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n",
        "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
        "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
        "    return unescape(text)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v34Pod6Wz0jY"
      },
      "source": [
        "To check if we are doing the conversion correctly or not, let's print html text first and then the converted text!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHDVLA-Tqnmg",
        "outputId": "384b3415-a40e-4986-c8c1-9c83b83b99a9"
      },
      "source": [
        "html_spam_emails = [email for email in X_train[y_train==1] if get_email_content(email) == \"text/html\"]\n",
        "random_html_spam = html_spam_emails[7]\n",
        "print(random_html_spam.get_content().strip()[:1500])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<HTML><HEAD><TITLE></TITLE><META http-equiv=\"Content-Type\" content=\"text/html; charset=windows-1252\"><STYLE>A:link {TEX-DECORATION: none}A:active {TEXT-DECORATION: none}A:visited {TEXT-DECORATION: none}A:hover {COLOR: #0033ff; TEXT-DECORATION: underline}</STYLE><META content=\"MSHTML 6.00.2713.1100\" name=\"GENERATOR\"></HEAD>\n",
            "<BODY text=\"#000000\" vLink=\"#0033ff\" link=\"#0033ff\" bgColor=\"#CCCC99\"><TABLE borderColor=\"#660000\" cellSpacing=\"0\" cellPadding=\"0\" border=\"0\" width=\"100%\"><TR><TD bgColor=\"#CCCC99\" valign=\"top\" colspan=\"2\" height=\"27\">\n",
            "<font size=\"6\" face=\"Arial, Helvetica, sans-serif\" color=\"#660000\">\n",
            "<b>OTC</b></font></TD></TR><TR><TD height=\"2\" bgcolor=\"#6a694f\">\n",
            "<font size=\"5\" face=\"Times New Roman, Times, serif\" color=\"#FFFFFF\">\n",
            "<b>&nbsp;Newsletter</b></font></TD><TD height=\"2\" bgcolor=\"#6a694f\"><div align=\"right\"><font color=\"#FFFFFF\">\n",
            "<b>Discover Tomorrow's Winners&nbsp;</b></font></div></TD></TR><TR><TD height=\"25\" colspan=\"2\" bgcolor=\"#CCCC99\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"5\"><tr><td valign=\"middle\" height=\"2\"><table width=\"300\" border=\"1\" cellspacing=\"0\" cellpadding=\"0\" bordercolor=\"#660000\" bgcolor=\"#EBEDCB\" height=\"5\" align=\"center\"><tr><td valign=\"middle\" height=\"8\"><div align=\"center\">\n",
            "<font face=\"Arial, Helvetica, sans-serif\" color=\"#660000\" size=\"5\">\n",
            "<b>For Immediate Release</b></font></div></td></tr></table></td></tr><tr><td><table width=\"100%\" border=\"1\" cellspacing=\"0\" cellpadding=\"8\" align=\"center\" bgcolor=\"#fafaf5\" borderco\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1ro5PeD0BbL",
        "outputId": "a4885e46-2ff8-4811-f8ca-50be5d761399"
      },
      "source": [
        "\n",
        "print(html_to_plain_text(random_html_spam.get_content())[:1500])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "OTC\n",
            " Newsletter\n",
            "Discover Tomorrow's Winners \n",
            "For Immediate Release\n",
            "Cal-Bay (Stock Symbol: CBYI)\n",
            "Watch for analyst \"Strong Buy Recommendations\" and several advisory newsletters picking CBYI.  CBYI has filed to be traded on the OTCBB, share prices historically INCREASE when companies get listed on this larger trading exchange. CBYI is trading around 25 cents and should skyrocket to $2.66 - $3.25 a share in the near future.\n",
            "Put CBYI on your watch list, acquire a position TODAY.\n",
            "REASONS TO INVEST IN CBYI\n",
            "A profitable company and is on track to beat ALL earnings estimates!\n",
            "One of the FASTEST growing distributors in environmental & safety equipment instruments.\n",
            "Excellent management team, several EXCLUSIVE contracts.  IMPRESSIVE client list including the U.S. Air Force, Anheuser-Busch, Chevron Refining and Mitsubishi Heavy Industries, GE-Energy & Environmental Research.\n",
            "RAPIDLY GROWING INDUSTRY\n",
            "Industry revenues exceed $900 million, estimates indicate that there could be as much as $25 billion from \"smell technology\" by the end of 2003.\n",
            "!!!!!CONGRATULATIONS!!!!!Our last recommendation to buy ORBT at $1.29 rallied and is holding steady at $3.50! Congratulations to all our subscribers that took advantage of this recommendation.\n",
            "ALL removes HONORED. Please allow 7 days to be removed and send ALL addresses to:\n",
            " HYPERLINK GoneForGood@btamail.net.cn\n",
            " \n",
            "Certain statements contained in this news release may be forward-looking statements within the meaning of The Private Securities Litigatio\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoPX38oz00dR"
      },
      "source": [
        "Now let's add these two functions into one so that it takes an email and converts it as a plain text!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cI6GCdv0QeC"
      },
      "source": [
        "def email_to_text(email):\n",
        "    html = None\n",
        "    for part in email.walk(): # walk() function iterates over the email where is_multipart is true\n",
        "        ctype = part.get_content_type()\n",
        "        if not ctype in (\"text/plain\", \"text/html\"):\n",
        "            continue\n",
        "        try:\n",
        "            content = part.get_content()\n",
        "        except: # in case of encoding issues\n",
        "            content = str(part.get_payload())\n",
        "        if ctype == \"text/plain\":\n",
        "            return content\n",
        "        else:\n",
        "            html = content\n",
        "    if html:\n",
        "        return html_to_plain_text(html)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcBF1Xog3o6y",
        "outputId": "561d5fa5-6af6-4292-8217-b0143bf485c3"
      },
      "source": [
        "print(email_to_text(random_html_spam)[:500])\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "OTC\n",
            " Newsletter\n",
            "Discover Tomorrow's Winners \n",
            "For Immediate Release\n",
            "Cal-Bay (Stock Symbol: CBYI)\n",
            "Watch for analyst \"Strong Buy Recommendations\" and several advisory newsletters picking CBYI.  CBYI has filed to be traded on the OTCBB, share prices historically INCREASE when companies get listed on this larger trading exchange. CBYI is trading around 25 cents and should skyrocket to $2.66 - $3.25 a share in the near future.\n",
            "Put CBYI on your watch list, acquire a position TODAY.\n",
            "REASONS TO INVEST I\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFO7qKSf4gGK"
      },
      "source": [
        "Now we use prepare the natural language stems using which we will detect if an email is spam or not!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YH7znPD4PQ3"
      },
      "source": [
        "stem = nltk.PorterStemmer()\n",
        "url_extractor = urlextract.URLExtract()\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCUW-aEP6Gzd"
      },
      "source": [
        "Now we will create a transformer class to fully procress the plain text of the emails. So that we can then train our models on those processed words!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDusMs_M5qW0"
      },
      "source": [
        "class EmailToWordTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, lower_case=True, remove_punctuation=True,\n",
        "                 replace_urls=True, replace_numbers=True, stemming=True):\n",
        "        self.lower_case = lower_case\n",
        "        self.remove_punctuation = remove_punctuation\n",
        "        self.replace_urls = replace_urls\n",
        "        self.replace_numbers = replace_numbers\n",
        "        self.stemming = stemming\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        X_transformed = []\n",
        "        for email in X:\n",
        "            text = email_to_text(email) or \"\"\n",
        "            if self.lower_case:\n",
        "                text = text.lower()\n",
        "\n",
        "            if self.replace_urls and url_extractor is not None:\n",
        "                urls = list(set(url_extractor.find_urls(text)))\n",
        "                urls.sort(key=lambda url: len(url), reverse=True)\n",
        "                for url in urls:\n",
        "                    text = text.replace(url, \" URL \")\n",
        "\n",
        "            if self.replace_numbers:\n",
        "                text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n",
        "\n",
        "            if self.remove_punctuation:\n",
        "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
        "            \n",
        "            word_counts = Counter(text.split())\n",
        "\n",
        "            if self.stemming and stem is not None:\n",
        "                stemmed_word_counts = Counter()\n",
        "                for word, count in word_counts.items():\n",
        "                    stemmed_word = stem.stem(word)\n",
        "                    stemmed_word_counts[stemmed_word] += count\n",
        "                word_counts = stemmed_word_counts\n",
        "            X_transformed.append(word_counts)\n",
        "        return np.array(X_transformed)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXyQC3vm7mtB",
        "outputId": "d8ee615d-d179-459a-cbdc-e387ffb353f6"
      },
      "source": [
        "check_x = X_train[:2]\n",
        "words_from_check = EmailToWordTransformer().fit_transform(check_x)\n",
        "words_from_check"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([Counter({'chuck': 1, 'murcko': 1, 'wrote': 1, 'stuff': 1, 'yawn': 1, 'r': 1}),\n",
              "       Counter({'the': 11, 'of': 9, 'and': 8, 'all': 3, 'christian': 3, 'to': 3, 'by': 3, 'jefferson': 2, 'i': 2, 'have': 2, 'superstit': 2, 'one': 2, 'on': 2, 'been': 2, 'ha': 2, 'half': 2, 'rogueri': 2, 'teach': 2, 'jesu': 2, 'some': 1, 'interest': 1, 'quot': 1, 'url': 1, 'thoma': 1, 'examin': 1, 'known': 1, 'word': 1, 'do': 1, 'not': 1, 'find': 1, 'in': 1, 'our': 1, 'particular': 1, 'redeem': 1, 'featur': 1, 'they': 1, 'are': 1, 'alik': 1, 'found': 1, 'fabl': 1, 'mytholog': 1, 'million': 1, 'innoc': 1, 'men': 1, 'women': 1, 'children': 1, 'sinc': 1, 'introduct': 1, 'burnt': 1, 'tortur': 1, 'fine': 1, 'imprison': 1, 'what': 1, 'effect': 1, 'thi': 1, 'coercion': 1, 'make': 1, 'world': 1, 'fool': 1, 'other': 1, 'hypocrit': 1, 'support': 1, 'error': 1, 'over': 1, 'earth': 1, 'six': 1, 'histor': 1, 'american': 1, 'john': 1, 'e': 1, 'remsburg': 1, 'letter': 1, 'william': 1, 'short': 1, 'again': 1, 'becom': 1, 'most': 1, 'pervert': 1, 'system': 1, 'that': 1, 'ever': 1, 'shone': 1, 'man': 1, 'absurd': 1, 'untruth': 1, 'were': 1, 'perpetr': 1, 'upon': 1, 'a': 1, 'larg': 1, 'band': 1, 'dupe': 1, 'import': 1, 'led': 1, 'paul': 1, 'first': 1, 'great': 1, 'corrupt': 1})],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ8kSxyP-Sqb"
      },
      "source": [
        "Until now everything looks good enough! Now we will convert these words into a vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ0WG_g78Fkw"
      },
      "source": [
        "class WordToVectorTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vocabulary_size=1000):\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "    def fit(self, X, y=None):\n",
        "        total_count = Counter()\n",
        "        for word_count in X:\n",
        "            for word, count in word_count.items():\n",
        "                total_count[word] += min(count, 10)\n",
        "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
        "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        rows = []\n",
        "        cols = []\n",
        "        data = []\n",
        "        for row, word_count in enumerate(X):\n",
        "            for word, count in word_count.items():\n",
        "                rows.append(row)\n",
        "                cols.append(self.vocabulary_.get(word, 0))\n",
        "                data.append(count)\n",
        "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size +1))"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3Nlm2VmDuEb",
        "outputId": "2e70e1e5-fa66-477e-e067-d2dc3d3c7947"
      },
      "source": [
        "vocab_transformer = WordToVectorTransformer(vocabulary_size=10)\n",
        "X_few_vectors = vocab_transformer.fit_transform(words_from_check)\n",
        "X_few_vectors.toarray()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [97, 11,  9,  8,  3,  3,  3,  3,  2,  2,  2]], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OprjIW8VGG4g"
      },
      "source": [
        "\n",
        "What does this matrix mean? Well, the 97 in the second row, first column, means that the second email contains 97 words that are not part of the vocabulary. The 11 next to it means that the first word in the vocabulary is present 11 times in this email. The 9 next to it means that the second word is present 9 times, and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvlIO9e6HL3l"
      },
      "source": [
        "#### **Step 4: Pipeline for Data Procressing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eQ26ObiHiMT"
      },
      "source": [
        "At this point you already understood the struggle is mainly in the data procressing part. That's why we AI Engineers love to make pipelines for the whole data procressing workflow. That makes easier for us to procress a raw data with just one line of code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QKOqWcCEnz-"
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    (\"email_to_word\", EmailToWordTransformer()),\n",
        "    (\"word_to_vector\", WordToVectorTransformer()),\n",
        "])\n",
        "\n",
        "X_train_transformed = pipeline.fit_transform(X_train) # We have processed our whole training data with just one line of code!"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V22HyFRII2T0"
      },
      "source": [
        "#### **Step 5: Train Your Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yyd6_NuuJDOg"
      },
      "source": [
        "We will use the logistic regression for this model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avLFQf9MIXcb",
        "outputId": "e151ea63-94ff-46d6-efab-0cd4f997495d"
      },
      "source": [
        "classifier = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
        "score = cross_val_score(classifier, X_train_transformed, y_train, cv=3, verbose=3, scoring=\"accuracy\")\n",
        "score"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  ................................................................\n",
            "[CV] .................................... , score=0.981, total=   0.2s\n",
            "[CV]  ................................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] .................................... , score=0.984, total=   0.3s\n",
            "[CV]  ................................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.5s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] .................................... , score=0.991, total=   0.5s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.98125, 0.98375, 0.99125])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTAojiE-KZc-"
      },
      "source": [
        "So we have got a 98% accuracy score here! This is really amazing. Now we will test our unseen data with this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOuVLiSqKkX7"
      },
      "source": [
        "#### **Step 6: Test Your Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "938zPHWsJsWJ",
        "outputId": "baa169f4-da1b-4ff7-8f71-fd901fc6d872"
      },
      "source": [
        "\n",
        "X_test_transformed = pipeline.transform(X_test)\n",
        "classifier.fit(X_train_transformed, y_train)\n",
        "y_pred = classifier.predict(X_test_transformed)\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(100 * cross_val_score(classifier, X_test_transformed, y_pred, cv=3, verbose=3, scoring=\"accuracy\").mean()))\n",
        "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
        "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))\n",
        "print(\"F1 Score: {:.2f}%\".format(100 * f1_score(y_test, y_pred)))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  ................................................................\n",
            "[CV] .................................... , score=0.985, total=   0.2s\n",
            "[CV]  ................................................................\n",
            "[CV] .................................... , score=0.950, total=   0.2s\n",
            "[CV]  ................................................................\n",
            "[CV] .................................... , score=0.975, total=   0.1s\n",
            "Accuracy: 97.00%\n",
            "Precision: 95.88%\n",
            "Recall: 97.89%\n",
            "F1 Score: 96.88%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.4s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by-BHEcjMV6d"
      },
      "source": [
        "## **The End**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFIc7h6GLyMI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}